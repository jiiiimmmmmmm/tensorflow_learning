### how to deal with the missing value?

1. drop columns with missing values. This is a simple measure
2. imputation. this is better
3. an extension to imputation. In other words, it will add several new columns to show which values are missing originally and we used imputer to fit them later.

```python
# a example

#find the columns with missing values
cols_with_missing = [col 
                   for col in X_train.columns
                   if X_train.col.isnull().any()]

# the easiest way: drop these columns
X_train_drop_cols_with_missing = X_train.drop(cols_with_missing, axis=1)

# the second way is to use imputation
from sklearn.impute import SimpleImputer

imputer = SimpleImputer()
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))
imputed_X_train.columns = X_train.columns

# here are several questions I want to ask
# what does 'fit_transform' mean actually?
# ans: this function contains both fit function and transform function
# fit function calculates the values which will later be replaced the missing value 
# transform function uses these values to replace the missing value

# why variable 'imputed_X_train' need to be assigned the attribute   'columns' from originally variable 'X_train'?
# ans: imputer function 'fit_transform' return ndarray, which means the variable 'imputed_X_train' loses the columns' information.


# the third way is to extent the dataframe
for col in cols_with_missing:
    X_train_plus[col+'_was_missing'] = X_train[col].isnull()
    
# then use imuter to fit the missing values just like the second way
```

```python
# calculate the number of missing values in each column
missing_val_count_by_column = (X_train.isnull().sum())
```

### how to deal with categorical variables

1. drop categorical variables
2. label encoding
3. one-hot encoding

```python
# here is an example
# get list of categorical variables
s = (X_train.dtypes == 'object')
object_cols = list(s[s].index)
# or
object_cols = [col 
               for col in X_train.columns
              if X_train[col].dtype == 'object' 

               
# the first way is drop these columns
drop_X_train = X_train.select_dtypes(exclude=['object'])

# the second way is to use label encoding
               
# Columns that can be safely label encoded
good_label_cols = [col for col in object_cols if 
                   set(X_train[col]) == set(X_valid[col])]
# Columns that cannot be safely label encoded
bad_label_cols = list(set(object_cols)-set(good_label_cols))
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEnconder()
for col in object_cols:
	label_X_train[col] = label_encoder.fit_transform(X_train[col])

# the third way is to use one-hot enconding
               
# Columns that will be one-hot encoded
low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]
# Columns that will be dropped from the dataset
high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))

from sklearn.preprocessing import OneHotEncoder

OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)
OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))
OH_cols_train.index = X_train.index

num_X_train = X_train.drop(object_cols, axis=1)
OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)
```

### pipeline

```python
categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and 
                        X_train_full[cname].dtype == "object"]

# Select numerical columns
numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Preprocessing for numerical data
numerical_transformer = SimpleImputer(strategy='constant')

# Preprocessing for categorical data
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Bundle preprocessing for numerical and categorical data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_estimators=100, random_state=0)

from sklearn.metrics import mean_absolute_error

# Bundle preprocessing and modeling code in a pipeline
my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', model)
                             ])

# Preprocessing of training data, fit model 
my_pipeline.fit(X_train, y_train)

# Preprocessing of validation data, get predictions
preds = my_pipeline.predict(X_valid)

# Evaluate the model
score = mean_absolute_error(y_valid, preds)
print('MAE:', score)

```

### cross_validation

```python
from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline, X, y,
                              cv=5,
                              scoring='neg_mean_absolute_error')
```

### XGBoost

```python
from xgboost import XGBRegressor
# important argument: n_estimators, early_stopping_rounds, learning_rate, n_jobs
# n_estimators: how many times to go through the modeling cycle
# early_stopping_rounds: Early stopping causes the model to stop iterating when the validation score stops improving, even if we aren't at the hard stop for n_estimators.
# learning_rate: multiply the predictions from each model by a small number (known as the learning rate) before adding them in.
# n_jobs: use parallelism to build your models faster
my_model = XGBRegressor(n_estimators=500, learning_rate = 0.05, n_jobs=-1)
my_model.fit(X_train, y_train,
            early_stopping_rounds=5, 
            eval_set=[(X_valid, y_valid)],
            verbose=False)
```

### data leakage

1. target leakage: your predictors include data that will not be available at the time you make predictions. In other word, it is about the issue of time. You can not include some data which is collected after the data which we needs to predict. 

2. train-test contamination: distinguish training data from validation data carelessly, when the validation data affects the preprocessing processes

  ![]()
